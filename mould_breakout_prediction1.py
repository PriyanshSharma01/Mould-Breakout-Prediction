# -*- coding: utf-8 -*-
"""Mould_Breakout_Prediction1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-UlQfQB4_Be_DqqbMolq5y4UHAhZJm5X
"""

import numpy as np
import pandas as pd
import random
import os
import tensorflow as tf

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns 
import matplotlib.pyplot as plt
# %matplotlib inline

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
seed = 42
seed_everything(seed)

dataset = pd.read_csv("/content/Raw Data (1).csv")

dataset.head(10)

dataset.describe()

#correlation matrix
corrmat = dataset.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);

k = 4 #number of variables for heatmap
cols = corrmat.nlargest(k, 'Alarm')['Alarm'].index
cm = np.corrcoef(dataset[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

sns.set()
sns.pairplot(dataset, height = 2.5)
plt.show();

fig, ax1 = plt.subplots(5,2, figsize=(22,18))
k = 0
columns = list(dataset.columns)
for i in range(5):
    for j in range(2):
            sns.histplot(dataset[columns[k]], ax = ax1[i][j])
            k += 1
plt.show()

from scipy.stats import norm, skew

feats = ['T1','T2','T3','T4','T5','T6','T7','T8','CS','ML']
skewed_feats = dataset[feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
print("\nSkew in numerical features: \n")
skewness = pd.DataFrame({'Skew' :skewed_feats})
skewness

skewness = skewness[abs(skewness) > 0.75]
print("There are {} skewed numerical features to Box Cox transform".format(skewness.shape[0]))

from scipy.special import boxcox1p
skewed_features = skewness.index
lam = 0.15
for feat in skewed_features:
    dataset[feat] = boxcox1p(dataset[feat], lam)

dataset.fillna(dataset.mean(),inplace=True)

X = dataset.iloc[:, 0:10].values
y = dataset.iloc[:, 10].values

dataset.isnull().sum()

X

y

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X ,y ,test_size=0.2,random_state = seed)

X_train.shape

X_test.shape

# from sklearn.preprocessing import StandardScaler
# sc = StandardScaler()

# X_train = sc.fit_transform(X_train)
# X_test = sc.fit_transform(X_test)

from tensorflow import keras
from tensorflow.keras import layers

X_train[0]

X_train.shape[1]

model = keras.Sequential([                     
    layers.Dense(16, activation='relu', input_shape=[X_train.shape[1]]),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(12, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.3),

    layers.Dense(1, activation='sigmoid')
])

model.compile(
    # optimizer= keras.optimizers.Adam(
    # learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,
    # name='Adam'),
    optimizer= 'Adam',
    loss='binary_crossentropy',
    metrics=['accuracy'],
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    batch_size=16,
    epochs=100,
    )

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot()
history_df.loc[:, ['accuracy', 'val_accuracy']].plot()

print("Minimum Test Loss: {:0.4f}".format(history_df['val_loss'].min()))
print("Highest Test Accuracy: {:0.4f}".format(history_df['val_accuracy'].max()))

# showing a plot of how Accurate the model is against the 
# test dataset
plt.figure(figsize=(20, 8))
plt.grid(True)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()

# showing a plot of the loss with respect to the number of epochs 
plt.figure(figsize=(20, 8))
plt.grid(True)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.show()

